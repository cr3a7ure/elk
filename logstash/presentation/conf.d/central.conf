input {
	beats {
		port => 5044
		client_inactivity_timeout => 3600
	}
}

filter {
	# orders the input data into general fields. 
	grok {
	    patterns_dir => ["/etc/logstash/conf.d/patterns"]
		#patterns_dir => ["C:/Users/waeber/elk/logstash/LOGSTASH_HOME/conf.d/patterns"]
	    match => { "message" => "%{SWISSBIBLOG}" }
		tag_on_failure => [ "_message_parse_failure" ]
	}
	
	# reformats the date and removes the old timestamp field.
	date {
		match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
		target => "@timestamp"
		remove_field => [ "timestamp" ]
    	}
	
	# Adds hour and weekday as a seperate field to each event to be able to sort them into 
	# hours and weekday's. Based on system time of the logstash host instead of time stamp of the event.
	# This is easier to do, but less accurate.
	# Careful: Logstash does not take system time, but UTC. So hour + 1 = actual time in Basel.
	mutate {
		add_field => { "hour" => "%{+HH}" }
		add_field => { "week_day" => "%{+EEEE}" }
	}
	
	# these are duplicated requests to build facet hierarchies and other stuff and are mostly not needed.
	if ([request] =~ /\/AJAX\/J(SON|son)/) {
	    # keep these AJAX/JSON requests as they are specific actions which should be recorded.
		if ([requestparam] =~ /method=get(Co|SameGenre|Subject|SameMovement)Author[s]?/) {
		
		} else {
		    drop { }
	    }	
	}

	# removes events without useragent. These are all actions not taken by users.
	if ([useragent] =~ /-/) {
       		drop { }
	}
	
	# filters out a few types of request which happen a lot but add no interesting information
        if ([message] =~ /(the\+art\+of\+computer\+programming)|(\/themes)|(^"-" - - )|(GET \/apple-touch-icon)|(GET \/favicon.ico)|(GET \/\?lng)|(GET \/" 200 19698 "-" "-")|(\/Cover\/Show)/) {
       		drop { }
        }
	
	# remove fields with just a '-' in it. This way kibana can more easily deal with not existing fields.
	if ([responsesize] =~ /-/) {
		mutate {
			remove_field => [ "responsesize" ]
		}
	}
	
	if ([referrer] =~ /-/) {
		mutate {
			remove_field => [ "referrer" ]
		}
	}

	if ([forwardip] =~ /-/) {
		mutate {
			remove_field => [ "forwardip" ]
		}
	}
	
	if ([session] =~ /-/) {
		mutate {
			remove_field => [ "session" ]
		}
	}
	
	# some fields have two ips separated by comma, pack them into an arry for geoip to not throw an error
	# however currently they are not actually processed. 
	# unknown why they are not processed and unknown why there are two IPs in the first place.
	if ([forwardip] =~ /,/) {
		mutate {
			gsub => [ "forwardip", " ", "" ]
			split => { "forwardip" => "," }
		}
	}

	# Looks up the ip in the database and creates fields with associated data.
	if ([forwardip] =~ /^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$/) {
		geoip {
			source => "forwardip"
			#database => "C:/Users/waeber/Desktop/logstash-5.3.3/vendor/bundle/jruby/1.9/gems/logstash-filter-geoip-4.0.4-java/vendor/GeoLite2-City.mmdb"
			database => "/etc/logstash/geoip/GeoLite2-City.mmdb"

			target => "geoip"
			fields => ["city_name", "country_name", "location"]
			add_tag => [ "apache-geoip" ]
			tag_on_failure => ["_geoip_lookup_failure"]
		}
	}
	
	# analyzes the user agent of the request and creates a user field
	# 
	# https://www.elastic.co/guide/en/logstash/current/plugins-filters-useragent.html
	# https://de.wikipedia.org/wiki/User_Agent
	if ([useragent]){
		useragent {
			source => [ "useragent" ]
			target => [ "userdata" ]
		}
	}
	
	# remove some fields whith information we don't really need
	# the add_field is required as there is a documented bug which makes the removal impossible otherwise.
	# leaves userdata.device, userdata.name & userdata.os_name.
	if ([userdata]){
		mutate {
			add_field => { "[userdata][os_major]" => "os_major_version" }
			remove_field => ["[userdata][os_major]", "[userdata][os_minor]", "[userdata][os]", "[userdata][patch]", "[userdata][major]", "[userdata][minor]", "[userdata][build]" ]
		}
	}

	# process the request fields. 
	if ([request]) {
		mutate {
			# keep adding fields and manipulating fields separate as they might not work otherwise.
			add_field => { "request_split" => "%{request}"}
		}
		
		mutate {
			# replace slashes with spaces to easily remove the leading slash and then split the path.
			gsub => [ "request_split", "/", " " ]
			strip => [ "request_split" ]
			split => { "request_split" => " "}
		}
		
		# move each request part into a separat field for better searching and visualizing.
		if ([request] =~ /^\/[\w.-]+$/) {
			grok {
				id => "single_request_parse"
				match => { "request" => "\/(?<request_root>.*?$)" }
				tag_on_failure => [ "_single_request_parse_failure" ]
			}
		}
		if ([request] =~ /^\/[\w.-]+\/[\w.-]+$/) {
			grok {
				id => "double_request_parse"
				match => { "request" => "\/(?<request_root>.*?)\/(?<request_middle>.*?$)" }
				tag_on_failure => [ "_double_request_parse_failure" ]
			}
		}
		if ([request] =~ /^\/[\w.-]+\/[\w.-]+\/[\w.-]+$/) {
			grok {
				id => "triple_request_parse"
				match => { "request" => "\/(?<request_root>.*?)\/(?<request_middle>.*?)\/(?<request_end>.*?$)" }
				tag_on_failure => [ "_triple_request_parse_failure" ]
			}
		}
		
		if ([request] =~ /^\/[\w.-]+\/[\w.-]+\/[\w.-]+\/[\w.-]+$/) {
			grok {
				id => "fourth_request_parse"
				match => { "request" => "\/(?<request_root>.*?)\/(?<request_middle>.*?)\/(?<request_end>.*?)\/(?<request_end_id>.*?$)" }
				tag_on_failure => [ "_fourth_request_parse_failure" ]
			}
		}
		
		# searches for holding requests and extracts the holding identifiers.
		# and stores the value in a new field 'library_code'
		if ([request_root] == "Holdings") {
			grok {
				id => "holdings_grok"
				match => { "request_end" => "%{WORD:library_code}" }
				tag_on_failure => [ "_holding_parse_failure" ]
			}	
		}
		
		# looks up the library code in dictionary and puts the name into library_name
		# requires manual update if new libraries are added to swissbib
		if ([library_code]) {
			translate {
				field => "library_code"
				destination => "library_name"
				dictionary_path => "/etc/logstash/conf.d/dictionaries/swissbib_libraries.yml"
				fallback => "Unknown"
				exact => true
				refresh_interval => 18000
			}
		}
	}
	
	if ([requestparam]) {
		mutate {
			add_field => { "requestparam_split" => "%{requestparam}"}			
		}
		mutate {
			split => { "requestparam_split" => "&"}
		}
	
		urldecode {
			all_fields => true
			charset => "UTF-8"
			tag_on_failure => ["_url_decode_failure"]
		}
	
		# parses request params and puts each assignment into a field.
		kv {
			source => "requestparam_split"
			target => "search_params"
			# whitlist of all included fields. if this is removed hundreds of fields are created.
			include_keys => [ "lookfor", "type", "sort", "filter", "bool", "edit", 
								"entityid", "expandlib", "facet", "join", "limit", "originalsort", 
								"page", "publicationdatefrom", "publicationdateto", "publishdatefrom", 
								"publishdateto", "style", "view", "language", "tracklang", "trackurl" ]
			# remove numbers to ensure that everything is mapped to the same field. 
			remove_char_key => "\[\]0123456789"
			transform_key => "lowercase"
			field_split => "?"
			value_split => "="
		}

		# translates single level expandlib requests.
		if ([search_params][expandlib]) {
			translate {
				field => "[search_params][expandlib]"
				destination => "[search_params][expandlib_library]"
				dictionary_path => "/etc/logstash/conf.d/dictionaries/swissbib_libraries.yml"
				fallback => "Unknown"
				exact => true
				refresh_interval => 18000
			}	
		}
		
		# if double level expandlib requests are encountered this will split them and translate them into human readable form and then throw away the temporary splited fields.
		if ([search_params][expandlib_library] == "Unknown") {
			grok {
				id => "expandlib_expansion"
				match => { "[search_params][expandlib]" => "%{DATA:[search_params][expandlib_region_temp]}-%{DATA:[search_params][expandlib_library_temp]}" }
				tag_on_failure => "_failed_to_split_expandlib"
			}
			
			translate {
				field => "[search_params][expandlib_region_temp]"
				destination => "[search_params][expandlib_region]"
				dictionary_path => "/etc/logstash/conf.d/dictionaries/swissbib_libraries.yml"
				exact => true
				override => true
				fallback => "Unknown"
				refresh_interval => 18000
			}
			
			translate {
				field => "[search_params][expandlib_library_temp]"
				destination => "[search_params][expandlib_library]"
				dictionary_path => "/etc/logstash/conf.d/dictionaries/swissbib_libraries.yml"
				exact => true
				override => true
				fallback => "Unknown"
				refresh_interval => 18000
			}
			
			mutate {
				remove_field => [ "[search_params][expandlib_library_temp]", "[search_params][expandlib_region_temp]" ]
			
			}
		}
		
		# expand facet filters into fields:
		if ([search_params][filter]) {
			# replace + characters with spaces to make it more readable.
			mutate {
				gsub => [ "[search_params][filter]", "\+", " " ]
			}
			kv {
				source => "[search_params][filter]"
				target => "filter_params"
				# precaution to not allow this to create additional fields which we do not expect. Can be created by bots.
				include_keys => ["filter_str_mv", "e_institution_str_mv", "contenttype", "format_hierarchy_str_mv", "format_str_mv", 
								"navsubform", "navsub_orange", "navsub_jus", "navsub_green", "subjectterms", "navauthor_full", "navauthor_orange", 
								"publishdate", "publicationdate", "navdrsys", "navdrsys_gen", "navdrsys_d", "navdrsys_e", "mylibrary", "institution", "union", "navsub_geofull",
								"discipline", "format", "includenewspapers", "includewithoutfulltext", "ispeerreviewed", "language", "lists", "tags"
								]
				remove_char_key => "~-"
				remove_char_value => "\\\""
				transform_key => "lowercase"
				value_split => ":"
			}
			# rename filters to a more readable form and more unified between platforms.
			mutate {
				# Online Available?
				rename => { "[filter_params][filter_str_mv]" => "[filter_params][online]" }
				rename => { "[filter_params][e_institution_str_mv]" => "[filter_params][online]" }
				# Format
				rename => { "[filter_params][contenttype]" => "[filter_params][format]" }
				rename => { "[filter_params][format_hierarchy_str_mv]" => "[filter_params][format]" }
				rename => { "[filter_params][format_str_mv]" => "[filter_params][format]" }
				# Genre Form
				rename => { "[filter_params][navsubform]" => "[filter_params][genre_form]" }
				# Subject
				rename => { "[filter_params][navsub_orange]" => "[filter_params][subject]" }
				rename => { "[filter_params][navsub_jus]" => "[filter_params][subject]" }
				rename => { "[filter_params][navsub_green]" => "[filter_params][subject]" }
				rename => { "[filter_params][subjectterms]" => "[filter_params][subject]" }
				# Author
				rename => { "[filter_params][navauthor_full]" => "[filter_params][author]" }
				rename => { "[filter_params][navauthor_orange]" => "[filter_params][author]" }
				# Publication Date				
				rename => { "[filter_params][publishdate]" => "[filter_params][publicationdate]" }
				# Jus Classification
				rename => { "[filter_params][navdrsys]" => "[filter_params][jus_classification]" }
				rename => { "[filter_params][navdrsys_gen]" => "[filter_params][advanced_jus_classification]" }
				rename => { "[filter_params][navdrsys_d]" => "[filter_params][advanced_jus_classification]" }
				rename => { "[filter_params][navdrsys_e]" => "[filter_params][advanced_jus_classification]" }
				# Libraries
				rename => { "[filter_params][mylibrary]" => "[filter_params][library]" }
				rename => { "[filter_params][institution]" => "[filter_params][library]" }
				# Library Networks
				rename => { "[filter_params][union]" => "[filter_params][library_network]" }
				# Geography
				rename => { "[filter_params][navsub_geofull]" => "[filter_params][geography]" }											
			}
			
			# IMPORTANT: Do not make these fields overwrite themselves, as the translate filter plugin cannot handle 
			# arrays of data. If it encounters an array it simply takes the first value and overwrites the rest.
			# POSSIBLE WORKAROUND: https://gist.github.com/human39/8b80d51a75c99e64eb42
			# translate library names from filters.
			if ([filter_params][library]) {
				translate {
					field => "[filter_params][library]"
					destination => "[filter_params][library_name]"
					dictionary_path => "/etc/logstash/conf.d/dictionaries/swissbib_libraries.yml"
					fallback => "Unknown"
					exact => true
					refresh_interval => 18000
				}	
			}
			# translate library network names from filter params.
			if ([filter_params][library_network]) {
				translate {
					field => "[filter_params][library_network]"
					destination => "[filter_params][library_network_name]"
					dictionary_path => "/etc/logstash/conf.d/dictionaries/union_translations.yml"
					fallback => "Unknown"
					exact => true
					refresh_interval => 18000
				}	
			}
			# translate library names from the online filter.
			if ([filter_params][online]) {
				translate {
					field => "[filter_params][online]"
					destination => "[filter_params][online_library_name]"
					dictionary_path => "/etc/logstash/conf.d/dictionaries/swissbib_libraries.yml"
					fallback => "Unknown"
					exact => true
					refresh_interval => 18000
				}	
			}
			# translate jus classifications 
			if ([filter_params][jus_classification]) {
				translate {
					field => "[filter_params][jus_classification]"
					destination => "[filter_params][jus_classification_name]"
					exact => true
					fallback => "Unknown"
					dictionary_path => "/etc/logstash/conf.d/dictionaries/jus_classifications.yml"
					refresh_interval => 18000
				}
			}
			# translate advanced jus classifications 
			if ([filter_params][advanced_jus_classification]) {
				translate {
					field => "[filter_params][advanced_jus_classification]"
					destination => "[filter_params][advanced_jus_classification_name]"
					exact => true
					fallback => "Unknown"
					dictionary_path => "/etc/logstash/conf.d/dictionaries/jus_classifications.yml"
					refresh_interval => 18000
				}
			}
			# translate format codes
			if ([filter_params][format]) {
				translate {
					field => "[filter_params][format]"
					destination => "[filter_params][format_name]"
					exact => true
					fallback => "Unknown"
					dictionary_path => "/etc/logstash/conf.d/dictionaries/swissbib_format_codes.yml"
					refresh_interval => 18000
				}
			}

			# rename shortened language codes.
			if ([filter_params][language]) {
				translate {
					field => "[filter_params][language]"
					destination => "[filter_params][language_name]"
					exact => true
					fallback => "Unknown"
					dictionary_path => "/etc/logstash/conf.d/dictionaries/iso_639_2_language_codes.yml"
					refresh_interval => 18000
				}
			}
			
			# extract publication dates.
			if ([filter_params][publicationdate]) {
				mutate {
					# simplifing the expression to have a simple working grok pattern to match.
					gsub => [ "[filter_params][publicationdate]", "\+", " " ]
					gsub => [ "[filter_params][publicationdate]", "\[", "" ]
					gsub => [ "[filter_params][publicationdate]", "\]", "" ]
					gsub => [ "[filter_params][publicationdate]", "\*", "00" ]
				}
				# extract dates into from and to fields.
				grok {
					id => "publication_date_grok"
					match => { "[filter_params][publicationdate]" => "%{YEAR:[filter_params][date_from]} TO %{YEAR:[filter_params][date_to]}" }
					tag_on_failure => "_publication_date_failure"
				}
				# remove publication date field as it is no longe needed.
				mutate {
					remove_field => [ "[filter_params][publicationdate]" ]
				}
			}
		}
		# translate sort orders into more human readable form.
		if ([search_params][sort]) {
			translate {
				field => "[search_params][sort]"
				destination => "[search_params][sort]"
				override => true
				exact => true
				dictionary => [
					"relevance", "Relevance",
					"publishDateSort+desc", "Publication Date (Descending)",
					"publishDateSort+asc", "Publication Date (Ascending)",
					"title", "Title",
					"title_sort+asc", "Title (Ascending)",
					"title_sort+desc", "Title (Descending)",		
					"author_sort+asc", "Author (Ascending)",
					"author_sort+desc", "Author (Descending)",
					"PublicationDate:asc", "Publication Date (Ascending)",
					"PublicationDate:desc", "Publication Date (Descending)",
					"publishDateSort desc", "Publication Date (Descending)",
					"publishDateSort asc", "Publication Date (Ascending)",
					"year+DESC", "Publication Date (Descending)",
					"year+ASC", "Publication Date (Ascending)"
				]
			}
		}
	
		#parse search strings
		if ([search_params][lookfor]) {
			# replace all + and ,.
			mutate {
				gsub => [ "[search_params][lookfor]", "\+", " " ]
				gsub => [ "[search_params][lookfor]", ",", "" ]
			}
			
			# Adds a copy of the search field.
			mutate {
				add_field => { "[search_params][search]" => "%{[search_params][lookfor]}"}
			}
			# when lookfor has several values (advanced searches) as an array they are concatenated with a comma and not space. 
			# this ensures that these commas are removed and then the fields can be properly split.
			mutate {
				gsub => [ "[search_params][search]", ",", " " ]
			}
			
			# splits the search field into an array.
			mutate {
				split => { "[search_params][search]" => " " }
			}
			
			# count the number of words in a search.
			if ([search_params][search]) {
				mutate {
					add_field => { "[search_params][words_in_search]" => "" }
				}
				ruby {
					code => "event.set('[search_params][words_in_search]', event.get('[search_params][search]').length)"
				}
			}
		}
		
		# lowercase this field as it contains the same values with different capitalizations depending on plattform and usage.
		# TODO: possibly add a translate filter to figure out which values have what meaning as some are cryptic shortcuts.
		if ([search_params][type]){
			mutate {
				lowercase => [ "[search_params][type]" ]
			}
		}
	}
	
	# detects and translates hosts
    if ([host] =~ /sb-uvf1|sb-uvf2|sb-uvf3/) {
		mutate {
			add_field => { "sourcetype" => "presentationgreen" } 
		}
    } 

    if ([host] =~ /sb-uvf9|sb-uvf10|sb-uvf11/) {
		mutate {
			add_field => {"sourcetype" => "presentationbb" } 
		}
    } 

    if ([host] =~ /sb-uvf5/) {
		mutate {
			add_field => {"sourcetype" => "presentationjus" } 
		}
    }
	
	# create a field with the full link except the start. Can be added in kibana with a url template.
	# to directly call the link in the browser and make the search...
	# three different fields, since there is no option to have multiple templates.
	# the space is added to ensure that add_fields filter works properly and then removed again...
	if ([sourcetype] == "presentationgreen") {
		if ([requestparam]) {
			mutate {
				add_field => { "link_green" => "%{request}%{requestparam}" }		
			}
		}
		else {
			mutate {
				add_field => { "link_green" => "%{request}" }		
			}
		}
	}
	
	if ([sourcetype] == "presentationbb") {
		if ([requestparam]) {
			mutate {
				add_field => { "link_bb" => "%{request}%{requestparam}" }		
			}
		}
		else {
			mutate {
				add_field => { "link_bb" => "%{request}" }		
			}
		}
	} 
	
	if ([sourcetype] == "presentationjus") {
		if ([requestparam]) {
			mutate {
				add_field => { "link_jus" => "%{request}%{requestparam}" }		
			}
		}
		else {
			mutate {
				add_field => { "link_jus" => "%{request}" }		
			}
		}
	}
}

output {
    if ([sourcetype] == "presentationgreen") {
		elasticsearch { 
			hosts => ["sb-uesl1.swissbib.unibas.ch:8080","sb-uesl2.swissbib.unibas.ch:8080","sb-uesl3.swissbib.unibas.ch:8080"]
			#template => "C:/Users/waeber/elk/logstash/LOGSTASH_HOME/conf.d/es_template/apache_template.json"
			template => "/etc/logstash/conf.d/es_template/apache_template.json"

			template_name => "apache_template"
			template_overwrite => true
			index => "swissbib-green-%{+YYYY}"
			document_type => "logs"
		}
    }

    if ([sourcetype] == "presentationbb") {
		elasticsearch { 
			hosts => ["sb-uesl1.swissbib.unibas.ch:8080","sb-uesl2.swissbib.unibas.ch:8080","sb-uesl3.swissbib.unibas.ch:8080"]
			#template => "C:/Users/waeber/elk/logstash/LOGSTASH_HOME/conf.d/es_template/apache_template.json"
			template => "/etc/logstash/conf.d/es_template/apache_template.json"
			template_name => "apache_template"
			template_overwrite => true
			index => "swissbib-bb-%{+YYYY}"
			document_type => "logs"
		}
    }

    if ([sourcetype] == "presentationjus") {
		elasticsearch { 
			hosts => ["sb-uesl1.swissbib.unibas.ch:8080","sb-uesl2.swissbib.unibas.ch:8080","sb-uesl3.swissbib.unibas.ch:8080"]
			#template => "C:/Users/waeber/elk/logstash/LOGSTASH_HOME/conf.d/es_template/apache_template.json"
			template => "/etc/logstash/conf.d/es_template/apache_template.json"
			template_name => "apache_template"
			template_overwrite => true
			index => "swissbib-jus-%{+YYYY}"
			document_type => "logs"
		}
    }
}
